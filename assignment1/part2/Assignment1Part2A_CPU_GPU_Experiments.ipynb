{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import load_model\n",
    "import os\n",
    "from time import time\n",
    "import datetime\n",
    "from keras.callbacks import TensorBoard\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir=\"logs/cpu/{}\".format(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_CPU=mp.cpu_count()\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=8,\\\n",
    "        inter_op_parallelism_threads=8, allow_soft_placement=True,\\\n",
    "        device_count = {'CPU' : num_CPU, 'GPU' : 0})\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Epoch 1/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 1.8925 - acc: 0.3020 - val_loss: 1.6201 - val_acc: 0.4110\n",
      "Epoch 2/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 1.5964 - acc: 0.4162 - val_loss: 1.4496 - val_acc: 0.4695\n",
      "Epoch 3/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 1.4753 - acc: 0.4664 - val_loss: 1.6155 - val_acc: 0.4455\n",
      "Epoch 4/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 1.3966 - acc: 0.4966 - val_loss: 1.2291 - val_acc: 0.5536\n",
      "Epoch 5/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 1.3216 - acc: 0.5285 - val_loss: 1.3016 - val_acc: 0.5484\n",
      "Epoch 6/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 1.2647 - acc: 0.5497 - val_loss: 1.1035 - val_acc: 0.6092\n",
      "Epoch 7/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 1.2105 - acc: 0.5703 - val_loss: 1.0897 - val_acc: 0.6175\n",
      "Epoch 8/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 1.1640 - acc: 0.5874 - val_loss: 1.1178 - val_acc: 0.6060\n",
      "Epoch 9/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 1.1252 - acc: 0.6010 - val_loss: 1.0137 - val_acc: 0.6383\n",
      "Epoch 10/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 1.0935 - acc: 0.6144 - val_loss: 1.0090 - val_acc: 0.6508\n",
      "Epoch 11/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 1.0655 - acc: 0.6245 - val_loss: 0.9409 - val_acc: 0.6644\n",
      "Epoch 12/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 1.0373 - acc: 0.6347 - val_loss: 0.9172 - val_acc: 0.6727\n",
      "Epoch 13/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 1.0164 - acc: 0.6410 - val_loss: 0.9146 - val_acc: 0.6799\n",
      "Epoch 14/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.9923 - acc: 0.6508 - val_loss: 0.8916 - val_acc: 0.6870\n",
      "Epoch 15/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.9724 - acc: 0.6590 - val_loss: 0.8931 - val_acc: 0.6920\n",
      "Epoch 16/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.9558 - acc: 0.6664 - val_loss: 0.8355 - val_acc: 0.7114\n",
      "Epoch 17/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.9445 - acc: 0.6711 - val_loss: 0.8546 - val_acc: 0.7037\n",
      "Epoch 18/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.9275 - acc: 0.6789 - val_loss: 0.7952 - val_acc: 0.7218\n",
      "Epoch 19/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.9159 - acc: 0.6802 - val_loss: 0.9081 - val_acc: 0.6957\n",
      "Epoch 20/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.9026 - acc: 0.6873 - val_loss: 0.8448 - val_acc: 0.7078\n",
      "Epoch 21/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.8952 - acc: 0.6894 - val_loss: 0.7923 - val_acc: 0.7260\n",
      "Epoch 22/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.8945 - acc: 0.6906 - val_loss: 0.7618 - val_acc: 0.7368\n",
      "Epoch 23/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.8847 - acc: 0.6944 - val_loss: 0.8025 - val_acc: 0.7215\n",
      "Epoch 24/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.8751 - acc: 0.6988 - val_loss: 0.7772 - val_acc: 0.7390\n",
      "Epoch 25/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.8625 - acc: 0.7018 - val_loss: 0.7821 - val_acc: 0.7282\n",
      "Epoch 26/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.8607 - acc: 0.7033 - val_loss: 0.8408 - val_acc: 0.7217\n",
      "Epoch 27/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.8549 - acc: 0.7045 - val_loss: 0.7557 - val_acc: 0.7419\n",
      "Epoch 28/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.8513 - acc: 0.7051 - val_loss: 0.7877 - val_acc: 0.7340\n",
      "Epoch 29/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.8456 - acc: 0.7121 - val_loss: 0.7756 - val_acc: 0.7374\n",
      "Epoch 30/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.8470 - acc: 0.7107 - val_loss: 0.7538 - val_acc: 0.7438\n",
      "Epoch 31/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.8347 - acc: 0.7139 - val_loss: 0.7004 - val_acc: 0.7628\n",
      "Epoch 32/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.8332 - acc: 0.7147 - val_loss: 0.7642 - val_acc: 0.7425\n",
      "Epoch 33/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.8266 - acc: 0.7149 - val_loss: 0.7234 - val_acc: 0.7576\n",
      "Epoch 34/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.8213 - acc: 0.7186 - val_loss: 0.7103 - val_acc: 0.7607\n",
      "Epoch 35/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.8260 - acc: 0.7174 - val_loss: 0.7783 - val_acc: 0.7334\n",
      "Epoch 36/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.8218 - acc: 0.7194 - val_loss: 0.8012 - val_acc: 0.7330\n",
      "Epoch 37/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.8180 - acc: 0.7228 - val_loss: 0.6976 - val_acc: 0.7656\n",
      "Epoch 38/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.8136 - acc: 0.7240 - val_loss: 0.7150 - val_acc: 0.7585\n",
      "Epoch 39/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.8124 - acc: 0.7219 - val_loss: 0.7233 - val_acc: 0.7545\n",
      "Epoch 40/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.8029 - acc: 0.7271 - val_loss: 0.7867 - val_acc: 0.7284\n",
      "Epoch 41/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.8051 - acc: 0.7270 - val_loss: 0.7572 - val_acc: 0.7446\n",
      "Epoch 42/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.8026 - acc: 0.7285 - val_loss: 0.7560 - val_acc: 0.7465\n",
      "Epoch 43/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7994 - acc: 0.7271 - val_loss: 0.6671 - val_acc: 0.7747\n",
      "Epoch 44/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7992 - acc: 0.7283 - val_loss: 0.6959 - val_acc: 0.7685\n",
      "Epoch 45/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.8012 - acc: 0.7300 - val_loss: 0.7148 - val_acc: 0.7651\n",
      "Epoch 46/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7945 - acc: 0.7318 - val_loss: 0.7130 - val_acc: 0.7609\n",
      "Epoch 47/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7918 - acc: 0.7331 - val_loss: 0.6888 - val_acc: 0.7701\n",
      "Epoch 48/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7950 - acc: 0.7317 - val_loss: 0.7769 - val_acc: 0.7386\n",
      "Epoch 49/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7917 - acc: 0.7317 - val_loss: 0.7368 - val_acc: 0.7580\n",
      "Epoch 50/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7912 - acc: 0.7347 - val_loss: 0.6670 - val_acc: 0.7794\n",
      "Epoch 51/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7886 - acc: 0.7344 - val_loss: 0.7054 - val_acc: 0.7703\n",
      "Epoch 52/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7851 - acc: 0.7342 - val_loss: 0.7338 - val_acc: 0.7543\n",
      "Epoch 53/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7894 - acc: 0.7356 - val_loss: 0.8135 - val_acc: 0.7331\n",
      "Epoch 54/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7822 - acc: 0.7375 - val_loss: 0.7758 - val_acc: 0.7360\n",
      "Epoch 55/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7869 - acc: 0.7359 - val_loss: 0.7206 - val_acc: 0.7699\n",
      "Epoch 56/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7827 - acc: 0.7388 - val_loss: 0.7230 - val_acc: 0.7612\n",
      "Epoch 57/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7844 - acc: 0.7381 - val_loss: 0.6886 - val_acc: 0.7768\n",
      "Epoch 58/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7767 - acc: 0.7375 - val_loss: 0.6873 - val_acc: 0.7723\n",
      "Epoch 59/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7818 - acc: 0.7372 - val_loss: 0.7368 - val_acc: 0.7574\n",
      "Epoch 60/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7832 - acc: 0.7381 - val_loss: 0.6994 - val_acc: 0.7703\n",
      "Epoch 61/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7796 - acc: 0.7378 - val_loss: 0.7516 - val_acc: 0.7455\n",
      "Epoch 62/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7815 - acc: 0.7362 - val_loss: 0.6999 - val_acc: 0.7660\n",
      "Epoch 63/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7797 - acc: 0.7390 - val_loss: 0.7508 - val_acc: 0.7456\n",
      "Epoch 64/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7812 - acc: 0.7383 - val_loss: 0.6722 - val_acc: 0.7789\n",
      "Epoch 65/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7778 - acc: 0.7393 - val_loss: 0.7873 - val_acc: 0.7376\n",
      "Epoch 66/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7798 - acc: 0.7380 - val_loss: 0.8473 - val_acc: 0.7362\n",
      "Epoch 67/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7709 - acc: 0.7402 - val_loss: 0.7635 - val_acc: 0.7477\n",
      "Epoch 68/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7769 - acc: 0.7399 - val_loss: 0.8217 - val_acc: 0.7275\n",
      "Epoch 69/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7751 - acc: 0.7404 - val_loss: 0.7097 - val_acc: 0.7657\n",
      "Epoch 70/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7731 - acc: 0.7406 - val_loss: 0.7175 - val_acc: 0.7679\n",
      "Epoch 71/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7743 - acc: 0.7404 - val_loss: 0.7394 - val_acc: 0.7543\n",
      "Epoch 72/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7782 - acc: 0.7405 - val_loss: 0.6382 - val_acc: 0.7879\n",
      "Epoch 73/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7732 - acc: 0.7416 - val_loss: 0.7655 - val_acc: 0.7681\n",
      "Epoch 74/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7781 - acc: 0.7384 - val_loss: 0.7298 - val_acc: 0.7627\n",
      "Epoch 75/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7804 - acc: 0.7403 - val_loss: 0.6639 - val_acc: 0.7790\n",
      "Epoch 76/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7785 - acc: 0.7407 - val_loss: 0.7733 - val_acc: 0.7347\n",
      "Epoch 77/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7787 - acc: 0.7381 - val_loss: 0.7045 - val_acc: 0.7649\n",
      "Epoch 78/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7794 - acc: 0.7401 - val_loss: 0.6983 - val_acc: 0.7675\n",
      "Epoch 79/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7823 - acc: 0.7397 - val_loss: 0.7299 - val_acc: 0.7546\n",
      "Epoch 80/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7757 - acc: 0.7408 - val_loss: 0.7302 - val_acc: 0.7691\n",
      "Epoch 81/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7811 - acc: 0.7410 - val_loss: 0.6926 - val_acc: 0.7727\n",
      "Epoch 82/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7856 - acc: 0.7374 - val_loss: 0.7000 - val_acc: 0.7628\n",
      "Epoch 83/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7811 - acc: 0.7395 - val_loss: 0.7468 - val_acc: 0.7681\n",
      "Epoch 84/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7853 - acc: 0.7386 - val_loss: 0.7306 - val_acc: 0.7471\n",
      "Epoch 85/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7885 - acc: 0.7393 - val_loss: 0.7112 - val_acc: 0.7654\n",
      "Epoch 86/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7814 - acc: 0.7400 - val_loss: 0.7848 - val_acc: 0.7371\n",
      "Epoch 87/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7850 - acc: 0.7376 - val_loss: 0.7481 - val_acc: 0.7492\n",
      "Epoch 88/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7826 - acc: 0.7392 - val_loss: 0.7345 - val_acc: 0.7576\n",
      "Epoch 89/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7860 - acc: 0.7372 - val_loss: 0.7426 - val_acc: 0.7507\n",
      "Epoch 90/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7958 - acc: 0.7363 - val_loss: 0.7365 - val_acc: 0.7584\n",
      "Epoch 91/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7938 - acc: 0.7371 - val_loss: 0.7466 - val_acc: 0.7575\n",
      "Epoch 92/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7893 - acc: 0.7387 - val_loss: 0.7240 - val_acc: 0.7610\n",
      "Epoch 93/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7913 - acc: 0.7372 - val_loss: 0.8871 - val_acc: 0.7238\n",
      "Epoch 94/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7943 - acc: 0.7357 - val_loss: 0.8144 - val_acc: 0.7439\n",
      "Epoch 95/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7980 - acc: 0.7343 - val_loss: 0.8569 - val_acc: 0.7296\n",
      "Epoch 96/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.7974 - acc: 0.7354 - val_loss: 0.8346 - val_acc: 0.7430\n",
      "Epoch 97/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.8007 - acc: 0.7341 - val_loss: 0.7309 - val_acc: 0.7615\n",
      "Epoch 98/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.8098 - acc: 0.7310 - val_loss: 0.7444 - val_acc: 0.7569\n",
      "Epoch 99/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.8017 - acc: 0.7331 - val_loss: 0.8879 - val_acc: 0.7136\n",
      "Epoch 100/100\n",
      "1563/1562 [==============================] - 83s 53ms/step - loss: 0.8083 - acc: 0.7312 - val_loss: 0.7184 - val_acc: 0.7620\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True, callbacks=[tensorboard])\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4,\n",
    "                        steps_per_epoch=len(x_train)/batch_size, \n",
    "                        callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 401us/step\n",
      "Test loss: 0.7184320919036865\n",
      "Test accuracy: 0.762\n",
      "CPU Runtime: 8308.942848443985\n"
     ]
    }
   ],
   "source": [
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "end = time()\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "print('CPU Runtime:', str(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num_CPU=mp.cpu_count()\n",
    "\n",
    "# config = tf.ConfigProto(intra_op_parallelism_threads=8,\\\n",
    "#         inter_op_parallelism_threads=8, allow_soft_placement=True,\\\n",
    "#         device_count = {'CPU' : num_CPU, 'GPU' : 1})\n",
    "# session = tf.Session(config=config)\n",
    "# K.set_session(session)\n",
    "\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gpu = Sequential()\n",
    "model_gpu.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model_gpu.add(Activation('relu'))\n",
    "model_gpu.add(Conv2D(32, (3, 3)))\n",
    "model_gpu.add(Activation('relu'))\n",
    "model_gpu.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_gpu.add(Dropout(0.25))\n",
    "\n",
    "model_gpu.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model_gpu.add(Activation('relu'))\n",
    "model_gpu.add(Conv2D(64, (3, 3)))\n",
    "model_gpu.add(Activation('relu'))\n",
    "model_gpu.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_gpu.add(Dropout(0.25))\n",
    "\n",
    "model_gpu.add(Flatten())\n",
    "model_gpu.add(Dense(512))\n",
    "model_gpu.add(Activation('relu'))\n",
    "model_gpu.add(Dropout(0.5))\n",
    "model_gpu.add(Dense(num_classes))\n",
    "model_gpu.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model_gpu.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Epoch 1/100\n",
      "1563/1562 [==============================] - 14s 9ms/step - loss: 1.8631 - acc: 0.3180 - val_loss: 1.5886 - val_acc: 0.4270\n",
      "Epoch 2/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 1.5920 - acc: 0.4181 - val_loss: 1.3756 - val_acc: 0.4981\n",
      "Epoch 3/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 1.4742 - acc: 0.4660 - val_loss: 1.2815 - val_acc: 0.5339\n",
      "Epoch 4/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 1.3904 - acc: 0.4999 - val_loss: 1.3028 - val_acc: 0.5452\n",
      "Epoch 5/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 1.3234 - acc: 0.5248 - val_loss: 1.2210 - val_acc: 0.5573\n",
      "Epoch 6/100\n",
      "1563/1562 [==============================] - 12s 8ms/step - loss: 1.2629 - acc: 0.5502 - val_loss: 1.2083 - val_acc: 0.5653\n",
      "Epoch 7/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 1.2105 - acc: 0.5703 - val_loss: 1.0402 - val_acc: 0.6312\n",
      "Epoch 8/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 1.1714 - acc: 0.5829 - val_loss: 1.0884 - val_acc: 0.6188\n",
      "Epoch 9/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 1.1374 - acc: 0.6005 - val_loss: 0.9766 - val_acc: 0.6568\n",
      "Epoch 10/100\n",
      "1563/1562 [==============================] - 14s 9ms/step - loss: 1.1053 - acc: 0.6118 - val_loss: 0.9898 - val_acc: 0.6514\n",
      "Epoch 11/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 1.0721 - acc: 0.6221 - val_loss: 1.0439 - val_acc: 0.6435\n",
      "Epoch 12/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 1.0460 - acc: 0.6336 - val_loss: 0.9650 - val_acc: 0.6647\n",
      "Epoch 13/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 1.0248 - acc: 0.6399 - val_loss: 0.8687 - val_acc: 0.6966\n",
      "Epoch 14/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 1.0034 - acc: 0.6489 - val_loss: 0.8696 - val_acc: 0.6962\n",
      "Epoch 15/100\n",
      "1563/1562 [==============================] - 14s 9ms/step - loss: 0.9834 - acc: 0.6557 - val_loss: 0.9620 - val_acc: 0.6750\n",
      "Epoch 16/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.9745 - acc: 0.6581 - val_loss: 0.8265 - val_acc: 0.7113\n",
      "Epoch 17/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.9582 - acc: 0.6666 - val_loss: 0.7964 - val_acc: 0.7254\n",
      "Epoch 18/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.9420 - acc: 0.6704 - val_loss: 0.8584 - val_acc: 0.7019\n",
      "Epoch 19/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.9302 - acc: 0.6766 - val_loss: 0.8647 - val_acc: 0.6992\n",
      "Epoch 20/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.9202 - acc: 0.6779 - val_loss: 0.8283 - val_acc: 0.7153\n",
      "Epoch 21/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.9084 - acc: 0.6860 - val_loss: 0.8052 - val_acc: 0.7248\n",
      "Epoch 22/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.9083 - acc: 0.6839 - val_loss: 0.8118 - val_acc: 0.7221\n",
      "Epoch 23/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.8964 - acc: 0.6907 - val_loss: 0.8184 - val_acc: 0.7145\n",
      "Epoch 24/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.8902 - acc: 0.6906 - val_loss: 0.7648 - val_acc: 0.7347\n",
      "Epoch 25/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.8869 - acc: 0.6942 - val_loss: 0.8100 - val_acc: 0.7137\n",
      "Epoch 26/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.8750 - acc: 0.7000 - val_loss: 0.7574 - val_acc: 0.7379\n",
      "Epoch 27/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.8711 - acc: 0.6997 - val_loss: 0.7799 - val_acc: 0.7313\n",
      "Epoch 28/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.8664 - acc: 0.7022 - val_loss: 0.7277 - val_acc: 0.7547\n",
      "Epoch 29/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.8662 - acc: 0.7046 - val_loss: 0.7492 - val_acc: 0.7395\n",
      "Epoch 30/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.8578 - acc: 0.7063 - val_loss: 0.7958 - val_acc: 0.7253\n",
      "Epoch 31/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.8560 - acc: 0.7067 - val_loss: 0.8139 - val_acc: 0.7199\n",
      "Epoch 32/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.8422 - acc: 0.7096 - val_loss: 0.7467 - val_acc: 0.7469\n",
      "Epoch 33/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.8434 - acc: 0.7124 - val_loss: 0.7577 - val_acc: 0.7434\n",
      "Epoch 34/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.8462 - acc: 0.7096 - val_loss: 0.8148 - val_acc: 0.7152\n",
      "Epoch 35/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.8388 - acc: 0.7139 - val_loss: 0.7467 - val_acc: 0.7431\n",
      "Epoch 36/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.8331 - acc: 0.7157 - val_loss: 0.7868 - val_acc: 0.7331\n",
      "Epoch 37/100\n",
      "1563/1562 [==============================] - 14s 9ms/step - loss: 0.8306 - acc: 0.7148 - val_loss: 0.7526 - val_acc: 0.7440\n",
      "Epoch 38/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.8322 - acc: 0.7161 - val_loss: 0.7381 - val_acc: 0.7504\n",
      "Epoch 39/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.8292 - acc: 0.7170 - val_loss: 0.7825 - val_acc: 0.7341\n",
      "Epoch 40/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.8292 - acc: 0.7183 - val_loss: 0.7085 - val_acc: 0.7604\n",
      "Epoch 41/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.8242 - acc: 0.7186 - val_loss: 0.7056 - val_acc: 0.7630\n",
      "Epoch 42/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.8165 - acc: 0.7231 - val_loss: 0.7953 - val_acc: 0.7374\n",
      "Epoch 43/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.8201 - acc: 0.7204 - val_loss: 0.8213 - val_acc: 0.7281\n",
      "Epoch 44/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.8114 - acc: 0.7238 - val_loss: 0.7355 - val_acc: 0.7522\n",
      "Epoch 45/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.8173 - acc: 0.7248 - val_loss: 0.7326 - val_acc: 0.7535\n",
      "Epoch 46/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.8130 - acc: 0.7237 - val_loss: 0.7541 - val_acc: 0.7509\n",
      "Epoch 47/100\n",
      "1563/1562 [==============================] - 14s 9ms/step - loss: 0.8048 - acc: 0.7276 - val_loss: 0.8309 - val_acc: 0.7158\n",
      "Epoch 48/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.8118 - acc: 0.7223 - val_loss: 0.7035 - val_acc: 0.7637\n",
      "Epoch 49/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.8066 - acc: 0.7272 - val_loss: 0.7470 - val_acc: 0.7575\n",
      "Epoch 50/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.8012 - acc: 0.7280 - val_loss: 0.7362 - val_acc: 0.7582\n",
      "Epoch 51/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.8041 - acc: 0.7284 - val_loss: 0.7537 - val_acc: 0.7459\n",
      "Epoch 52/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.7977 - acc: 0.7320 - val_loss: 0.6973 - val_acc: 0.7629\n",
      "Epoch 53/100\n",
      "1563/1562 [==============================] - 14s 9ms/step - loss: 0.8006 - acc: 0.7293 - val_loss: 0.6773 - val_acc: 0.7690\n",
      "Epoch 54/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.7963 - acc: 0.7318 - val_loss: 0.8244 - val_acc: 0.7401\n",
      "Epoch 55/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.8023 - acc: 0.7302 - val_loss: 0.7281 - val_acc: 0.7649\n",
      "Epoch 56/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.7963 - acc: 0.7309 - val_loss: 0.7709 - val_acc: 0.7417\n",
      "Epoch 57/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.7925 - acc: 0.7306 - val_loss: 0.8032 - val_acc: 0.7415\n",
      "Epoch 58/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.7924 - acc: 0.7334 - val_loss: 0.7459 - val_acc: 0.7546\n",
      "Epoch 59/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.7922 - acc: 0.7351 - val_loss: 0.7286 - val_acc: 0.7583\n",
      "Epoch 60/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.7927 - acc: 0.7345 - val_loss: 0.6942 - val_acc: 0.7683\n",
      "Epoch 61/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.7876 - acc: 0.7340 - val_loss: 0.7540 - val_acc: 0.7551\n",
      "Epoch 62/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.7899 - acc: 0.7334 - val_loss: 0.7404 - val_acc: 0.7662\n",
      "Epoch 63/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.7899 - acc: 0.7359 - val_loss: 0.7381 - val_acc: 0.7519\n",
      "Epoch 64/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.7870 - acc: 0.7357 - val_loss: 0.7508 - val_acc: 0.7446\n",
      "Epoch 65/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.7897 - acc: 0.7356 - val_loss: 0.7615 - val_acc: 0.7691\n",
      "Epoch 66/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.7819 - acc: 0.7363 - val_loss: 0.8476 - val_acc: 0.7097\n",
      "Epoch 67/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.7890 - acc: 0.7348 - val_loss: 0.7437 - val_acc: 0.7653\n",
      "Epoch 68/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.7886 - acc: 0.7347 - val_loss: 0.7405 - val_acc: 0.7575\n",
      "Epoch 69/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.7879 - acc: 0.7355 - val_loss: 0.7328 - val_acc: 0.7570\n",
      "Epoch 70/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.7839 - acc: 0.7365 - val_loss: 0.7804 - val_acc: 0.7338\n",
      "Epoch 71/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.7795 - acc: 0.7402 - val_loss: 0.7342 - val_acc: 0.7546\n",
      "Epoch 72/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.7848 - acc: 0.7363 - val_loss: 0.7323 - val_acc: 0.7644\n",
      "Epoch 73/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.7881 - acc: 0.7383 - val_loss: 0.7661 - val_acc: 0.7553\n",
      "Epoch 74/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.7852 - acc: 0.7380 - val_loss: 0.7366 - val_acc: 0.7532\n",
      "Epoch 75/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.7915 - acc: 0.7354 - val_loss: 0.6805 - val_acc: 0.7777\n",
      "Epoch 76/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.7866 - acc: 0.7377 - val_loss: 0.7916 - val_acc: 0.7395\n",
      "Epoch 77/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.7878 - acc: 0.7379 - val_loss: 0.7819 - val_acc: 0.7552\n",
      "Epoch 78/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.7907 - acc: 0.7375 - val_loss: 0.7740 - val_acc: 0.7369\n",
      "Epoch 79/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.7859 - acc: 0.7373 - val_loss: 0.7265 - val_acc: 0.7540\n",
      "Epoch 80/100\n",
      "1563/1562 [==============================] - 14s 9ms/step - loss: 0.7870 - acc: 0.7352 - val_loss: 0.7186 - val_acc: 0.7610\n",
      "Epoch 81/100\n",
      "1563/1562 [==============================] - 14s 9ms/step - loss: 0.7964 - acc: 0.7353 - val_loss: 0.7680 - val_acc: 0.7441\n",
      "Epoch 82/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.7885 - acc: 0.7379 - val_loss: 0.7447 - val_acc: 0.7613\n",
      "Epoch 83/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.7887 - acc: 0.7373 - val_loss: 0.7147 - val_acc: 0.7730\n",
      "Epoch 84/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.7906 - acc: 0.7358 - val_loss: 0.7161 - val_acc: 0.7655\n",
      "Epoch 85/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.7919 - acc: 0.7364 - val_loss: 0.7750 - val_acc: 0.7366\n",
      "Epoch 86/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.7892 - acc: 0.7371 - val_loss: 0.7943 - val_acc: 0.7515\n",
      "Epoch 87/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.7931 - acc: 0.7365 - val_loss: 0.7832 - val_acc: 0.7381\n",
      "Epoch 88/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.7917 - acc: 0.7374 - val_loss: 0.7587 - val_acc: 0.7454\n",
      "Epoch 89/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.7953 - acc: 0.7362 - val_loss: 0.7328 - val_acc: 0.7625\n",
      "Epoch 90/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.7917 - acc: 0.7374 - val_loss: 0.7065 - val_acc: 0.7636\n",
      "Epoch 91/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.7963 - acc: 0.7372 - val_loss: 0.7988 - val_acc: 0.7397\n",
      "Epoch 92/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.7947 - acc: 0.7351 - val_loss: 0.6876 - val_acc: 0.7711\n",
      "Epoch 93/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.7969 - acc: 0.7348 - val_loss: 0.8777 - val_acc: 0.7056\n",
      "Epoch 94/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.8006 - acc: 0.7337 - val_loss: 0.7714 - val_acc: 0.7503\n",
      "Epoch 95/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.7995 - acc: 0.7344 - val_loss: 0.7389 - val_acc: 0.7539\n",
      "Epoch 96/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.8008 - acc: 0.7346 - val_loss: 0.7856 - val_acc: 0.7392\n",
      "Epoch 97/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.8073 - acc: 0.7309 - val_loss: 0.8779 - val_acc: 0.7116\n",
      "Epoch 98/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.8059 - acc: 0.7308 - val_loss: 0.7231 - val_acc: 0.7537\n",
      "Epoch 99/100\n",
      "1563/1562 [==============================] - 13s 9ms/step - loss: 0.8091 - acc: 0.7335 - val_loss: 0.6981 - val_acc: 0.7716\n",
      "Epoch 100/100\n",
      "1563/1562 [==============================] - 13s 8ms/step - loss: 0.8130 - acc: 0.7292 - val_loss: 0.7380 - val_acc: 0.7584\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model_gpu.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True, callbacks=[tensorboard])\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model_gpu.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4,\n",
    "                        steps_per_epoch=len(x_train)/batch_size,\n",
    "                        callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 143us/step\n",
      "Test loss: 0.7379762294769288\n",
      "Test accuracy: 0.7584\n",
      "GPU Runtime: 1331.109585762024\n"
     ]
    }
   ],
   "source": [
    "model_name = 'gpu_keras_cifar10_trained_model.h5'\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model_gpu.save(model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model_gpu.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "end = time()\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "print('GPU Runtime:', str(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
